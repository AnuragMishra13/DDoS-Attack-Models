Results for ADA BOOST : 
              precision    recall  f1-score   support

           0       0.99      0.75      0.85     35387
           1       1.00      1.00      1.00   4002078

    accuracy                           1.00   4037465
   macro avg       0.99      0.87      0.92   4037465
weighted avg       1.00      1.00      1.00   4037465

=====================================================
Results for CATBOOST : 
              precision    recall  f1-score   support

           0       0.95      0.94      0.95     35387
           1       1.00      1.00      1.00   4002078

    accuracy                           1.00   4037465
   macro avg       0.98      0.97      0.97   4037465
weighted avg       1.00      1.00      1.00   4037465

=====================================================
Results for DECISION TREE : 
              precision    recall  f1-score   support

           0       0.89      0.73      0.80     35387
           1       1.00      1.00      1.00   4002078

    accuracy                           1.00   4037465
   macro avg       0.94      0.87      0.90   4037465
weighted avg       1.00      1.00      1.00   4037465

=====================================================
Results for GRADIENT BOOSTING : 
              precision    recall  f1-score   support

           0       0.99      0.83      0.90     35387
           1       1.00      1.00      1.00   4002078

    accuracy                           1.00   4037465
   macro avg       0.99      0.91      0.95   4037465
weighted avg       1.00      1.00      1.00   4037465

=====================================================
Results for LGBM : 
              precision    recall  f1-score   support

           0       1.00      0.71      0.83     35387
           1       1.00      1.00      1.00   4002078

    accuracy                           1.00   4037465
   macro avg       1.00      0.85      0.91   4037465
weighted avg       1.00      1.00      1.00   4037465

=====================================================
Results for LOGISTIC REGRESSION : 
              precision    recall  f1-score   support

           0       0.84      1.00      0.91     35387
           1       1.00      1.00      1.00   4002078

    accuracy                           1.00   4037465
   macro avg       0.92      1.00      0.96   4037465
weighted avg       1.00      1.00      1.00   4037465

=====================================================
Results for NAIVE BAYES : 
              precision    recall  f1-score   support

           0       0.02      1.00      0.04     35387
           1       1.00      0.53      0.69   4002078

    accuracy                           0.53   4037465
   macro avg       0.51      0.76      0.36   4037465
weighted avg       0.99      0.53      0.68   4037465

=====================================================
Results for RANDOM FOREST : 
              precision    recall  f1-score   support

           0       1.00      0.90      0.94     35387
           1       1.00      1.00      1.00   4002078

    accuracy                           1.00   4037465
   macro avg       1.00      0.95      0.97   4037465
weighted avg       1.00      1.00      1.00   4037465

=====================================================
Results for XGBOOST : 
              precision    recall  f1-score   support

           0       1.00      0.91      0.95     35387
           1       1.00      1.00      1.00   4002078

    accuracy                           1.00   4037465
   macro avg       1.00      0.95      0.98   4037465
weighted avg       1.00      1.00      1.00   4037465

=====================================================




CATBOOST : 

/home/cryon/Projects/Final-Year-Project/DDoS/syn-parameter.py:21: DtypeWarning: Columns (85) have mixed types. Specify dtype option on import or set low_memory=False.
  train_df = pd.read_csv("Syn.csv").drop(columns=['Unnamed: 0', ' Timestamp', 'SimillarHTTP', 'Flow ID', ' Source IP', ' Destination IP'])
/home/cryon/Projects/Final-Year-Project/DDoS/syn-parameter.py:54: DtypeWarning: Columns (85) have mixed types. Specify dtype option on import or set low_memory=False.
  test_df = pd.read_csv("Testing_datasets/Syn.csv").drop(columns=['Unnamed: 0', ' Timestamp', 'SimillarHTTP', 'Flow ID', ' Source IP', ' Destination IP'])
[I 2024-09-16 20:09:26,769] A new study created in memory with name: no-name-b731402c-9633-4dd6-8511-c7575a1d84dc
[I 2024-09-16 20:10:12,667] Trial 0 finished with value: 0.9995222082930484 and parameters: {'learning_rate': 0.2366579999562485, 'depth': 6, 'n_estimators': 205, 'l2_leaf_reg': 1.661559099802478, 'boosting_type': 'Plain'}. Best is trial 0 with value: 0.9995222082930484.
[I 2024-09-16 20:10:59,316] Trial 1 finished with value: 0.9986791167299361 and parameters: {'learning_rate': 0.07141774298062953, 'depth': 5, 'n_estimators': 301, 'l2_leaf_reg': 1.1078831861883587, 'boosting_type': 'Plain'}. Best is trial 0 with value: 0.9995222082930484.
[I 2024-09-16 20:11:55,252] Trial 2 finished with value: 0.9999479922631098 and parameters: {'learning_rate': 0.2025501646477492, 'depth': 5, 'n_estimators': 258, 'l2_leaf_reg': 2.846674015362996, 'boosting_type': 'Ordered'}. Best is trial 2 with value: 0.9999479922631098.
[I 2024-09-16 20:12:41,563] Trial 3 finished with value: 0.9985042131366292 and parameters: {'learning_rate': 0.23507827811220078, 'depth': 4, 'n_estimators': 146, 'l2_leaf_reg': 0.7211668658856657, 'boosting_type': 'Plain'}. Best is trial 2 with value: 0.9999479922631098.
[I 2024-09-16 20:13:27,239] Trial 4 finished with value: 0.9992623178668001 and parameters: {'learning_rate': 0.14401978593211587, 'depth': 6, 'n_estimators': 102, 'l2_leaf_reg': 3.6196161807257394, 'boosting_type': 'Plain'}. Best is trial 2 with value: 0.9999479922631098.
Best hyperparameters: {'learning_rate': 0.2025501646477492, 'depth': 5, 'n_estimators': 258, 'l2_leaf_reg': 2.846674015362996, 'boosting_type': 'Ordered'}
Best F1 score: 0.9999


